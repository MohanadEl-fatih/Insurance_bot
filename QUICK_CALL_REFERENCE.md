# Quick Call Reference - Function Names in Sequence

## Complete Request Path (Service â†’ Method)

```
USER ACTION
  â†“
Frontend::ChatWindow.handleSendMessage(content)
  â†“
Frontend::fetch('/api/chat', {message})
  â†“
Next.js::POST(request)
  â”œâ”€ request.json()
  â”œâ”€ request.cookies.get('sid')
  â””â”€ fetch('http://localhost:8000/chat')
     â†“
FastAPI::chat(request, response, sid)
  â”œâ”€ uuid.uuid4()                       6   # Generate session ID
  â”œâ”€ response.set_cookie('sid')             # Set session cookie
  â””â”€ ChatService.process_message(session_id, message)
     â†“
ChatService::process_message(session_id, message)
  â”œâ”€ get_memory(session_id, redis_url)
  â”‚  â””â”€ RedisChatMessageHistory(session_id, url, ttl)
  â”‚     â””â”€ Redis::GET session:<uuid>        # Fetch chat history
  â”‚
  â”œâ”€ create_agent_executor(session_id)
  â”‚  â”œâ”€ get_llm()                           # Get LLM instance
  â”‚  â”œâ”€ get_tools()                         # Get [vehicle_lookup, get_quote]
  â”‚  â”œâ”€ ChatPromptTemplate.from_messages()  # Create prompt
  â”‚  â”œâ”€ create_openai_tools_agent()         # Create agent
  â”‚  â””â”€ AgentExecutor()                     # Create executor
  â”‚
  â”œâ”€ memory.messages                         # Get chat history
  â”‚
  â””â”€ agent_executor.ainvoke({input, chat_history})
     â†“
LangChain::AgentExecutor.ainvoke()
  â”œâ”€ Format prompt with system + history + input
  â”‚
  â”œâ”€ LLM::POST /v1/chat/completions         # FIRST LLM CALL
  â”‚  â””â”€ Response: tool_call(mock_vehicle_lookup)
  â”‚
  â”œâ”€ Execute tool: mock_vehicle_lookup(make, model, year)
  â”‚  â””â”€ VehicleService.lookup_vehicle(vin, make, model, year)
  â”‚     â”œâ”€ logger.info()
  â”‚     â””â”€ return {vin, make, model, year, status}
  â”‚
  â”œâ”€ LLM::POST /v1/chat/completions         # SECOND LLM CALL (with vehicle data)
  â”‚  â””â”€ Response: tool_call(mock_get_quote)
  â”‚
  â”œâ”€ Execute tool: mock_get_quote(vehicle_make, vehicle_model, vehicle_year, coverage_type)
  â”‚  â””â”€ QuoteService.get_quotes(vehicle_make, vehicle_model, vehicle_year, coverage_type)
  â”‚     â”œâ”€ logger.info()
  â”‚     â”œâ”€ Calculate base_premium
  â”‚     â”œâ”€ Generate 3 quotes with variations
  â”‚     â””â”€ return [quote1, quote2, quote3]
  â”‚
  â”œâ”€ LLM::POST /v1/chat/completions         # THIRD LLM CALL (with all tool results)
  â”‚  â””â”€ Response: final_text
  â”‚
  â””â”€ return {output: final_text}
     â†“
ChatService::process_message (continued)
  â”œâ”€ result.get('output')                    # Extract response text
  â”œâ”€ memory.add_user_message(message)
  â”‚  â””â”€ Redis::RPUSH session:<uuid> + EXPIRE 86400
  â”œâ”€ memory.add_ai_message(response)
  â”‚  â””â”€ Redis::RPUSH session:<uuid> + EXPIRE 86400
  â””â”€ return response
     â†“
FastAPI::chat (continued)
  â””â”€ return ChatResponse(message, meta)
     â†“
Next.js::POST (continued)
  â”œâ”€ response.json()
  â”œâ”€ NextResponse.json(data)
  â”œâ”€ nextResponse.headers.set('set-cookie')
  â””â”€ return nextResponse
     â†“
Frontend::ChatWindow.handleSendMessage (continued)
  â”œâ”€ response.json()
  â”œâ”€ setMessages([...prev, assistantMessage])
  â”œâ”€ setIsLoading(false)
  â””â”€ scrollToBottom()
     â†“
UI UPDATED - User sees response
```

---

## Service Layer Call Map

### 1. API Layer
```
FastAPI Router (/chat)
  â†’ chat(request, response, sid)
    â†’ ChatService.process_message()
```

### 2. Service Layer
```
ChatService
  â†’ process_message(session_id, message)
    â†’ Returns: AI response text

VehicleService
  â†’ lookup_vehicle(vin, make, model, year)
    â†’ Returns: {vin, make, model, year, status}

QuoteService
  â†’ get_quotes(vehicle_make, vehicle_model, vehicle_year, coverage_type)
    â†’ Returns: [quote1, quote2, quote3]
```

### 3. Agent Layer
```
agent_factory
  â†’ create_agent_executor(session_id)
    â†’ get_llm() â†’ Returns ChatOpenAI instance
    â†’ get_tools() â†’ Returns [mock_vehicle_lookup, mock_get_quote]
    â†’ Returns: AgentExecutor

tools
  â†’ mock_vehicle_lookup(**kwargs)
    â†’ VehicleService.lookup_vehicle()

  â†’ mock_get_quote(**kwargs)
    â†’ QuoteService.get_quotes()
```

### 4. Memory Layer
```
redis
  â†’ get_memory(session_id, redis_url)
    â†’ RedisChatMessageHistory()
    â†’ Returns: memory instance with methods:
      - .messages (get history)
      - .add_user_message(msg)
      - .add_ai_message(msg)
```

---

## Key Methods by File

### Frontend
| File | Method | Purpose |
|------|--------|---------|
| ChatWindow.tsx | handleSendMessage(content) | Handle user input |
| ChatWindow.tsx | setMessages(messages) | Update UI state |
| ChatWindow.tsx | setIsLoading(bool) | Toggle loading state |
| ChatWindow.tsx | scrollToBottom() | Auto-scroll chat |

### Next.js Proxy
| File | Method | Purpose |
|------|--------|---------|
| route.ts | POST(request) | Proxy requests to backend |
| route.ts | request.json() | Parse request body |
| route.ts | request.cookies.get('sid') | Extract session cookie |

### Backend API
| File | Method | Purpose |
|------|--------|---------|
| chat.py | chat(request, response, sid) | Main chat endpoint |
| chat.py | uuid.uuid4() | Generate session ID |
| chat.py | response.set_cookie() | Set session cookie |

### Services
| File | Method | Purpose |
|------|--------|---------|
| chat_service.py | ChatService.process_message() | Orchestrate chat flow |
| vehicle_service.py | VehicleService.lookup_vehicle() | Get vehicle info |
| quote_service.py | QuoteService.get_quotes() | Get insurance quotes |

### Agent
| File | Method | Purpose |
|------|--------|---------|
| agent_factory.py | create_agent_executor() | Create LangChain agent |
| agent_factory.py | get_llm() | Get LLM instance |
| tools.py | get_tools() | Get tool list |
| tools.py | mock_vehicle_lookup() | Vehicle lookup tool |
| tools.py | mock_get_quote() | Quote tool |

### Memory
| File | Method | Purpose |
|------|--------|---------|
| redis.py | get_memory() | Get Redis memory instance |

---

## LLM Call Sequence

```
1. agent_executor.ainvoke()
   â†“
2. POST http://localhost:1234/v1/chat/completions
   Payload: {messages: [system, history, user_input], tools: [...]}
   Response: {tool_calls: [mock_vehicle_lookup(...)]}
   â†“
3. Execute: VehicleService.lookup_vehicle()
   â†“
4. POST http://localhost:1234/v1/chat/completions
   Payload: {messages: [...previous, tool_result], tools: [...]}
   Response: {tool_calls: [mock_get_quote(...)]}
   â†“
5. Execute: QuoteService.get_quotes()
   â†“
6. POST http://localhost:1234/v1/chat/completions
   Payload: {messages: [...previous, tool_result], tools: [...]}
   Response: {content: "Great! I found quotes..."}
   â†“
7. Return final response
```

---

## Redis Operations

```
1. GET session:<session_id>
   â†’ Retrieve chat history

2. RPUSH session:<session_id> <user_message>
   â†’ Append user message

3. EXPIRE session:<session_id> 86400
   â†’ Reset TTL to 24 hours

4. RPUSH session:<session_id> <ai_message>
   â†’ Append AI message

5. EXPIRE session:<session_id> 86400
   â†’ Reset TTL to 24 hours
```

---

## HTTP Endpoints Called

```
1. POST http://localhost:3000/api/chat
   Frontend â†’ Next.js Proxy

2. POST http://localhost:8000/chat
   Next.js Proxy â†’ FastAPI Backend

3. POST http://localhost:1234/v1/chat/completions (Ã—3)
   Backend â†’ LM Studio LLM
```

---

## Complete Function Call Chain (One Line Per Call)

```
1.  ChatWindow.handleSendMessage()
2.  ChatWindow.setMessages()
3.  ChatWindow.setIsLoading(true)
4.  fetch('/api/chat')
5.  route.POST()
6.  request.json()
7.  request.cookies.get('sid')
8.  fetch('http://localhost:8000/chat')
9.  chat()
10. uuid.uuid4()
11. response.set_cookie()
12. ChatService.process_message()
13. get_memory()
14. RedisChatMessageHistory()
15. create_agent_executor()
16. get_llm()
17. get_tools()
18. ChatPromptTemplate.from_messages()
19. create_openai_tools_agent()
20. AgentExecutor()
21. memory.messages (Redis GET)
22. agent_executor.ainvoke()
23. [LangChain formats prompt]
24. POST /v1/chat/completions (Call #1)
25. mock_vehicle_lookup()
26. VehicleService.lookup_vehicle()
27. POST /v1/chat/completions (Call #2)
28. mock_get_quote()
29. QuoteService.get_quotes()
30. POST /v1/chat/completions (Call #3)
31. result.get('output')
32. memory.add_user_message() (Redis RPUSH)
33. memory.add_ai_message() (Redis RPUSH)
34. ChatResponse()
35. response.json()
36. NextResponse.json()
37. nextResponse.headers.set()
38. response.json()
39. ChatWindow.setMessages()
40. ChatWindow.setIsLoading(false)
41. ChatWindow.scrollToBottom()
```

---

## Summary by Component

| Component | Entry Method | Exit Method | Return Value |
|-----------|--------------|-------------|--------------|
| Frontend | handleSendMessage | scrollToBottom | void |
| Next.js Proxy | POST | return NextResponse | NextResponse |
| FastAPI | chat | return ChatResponse | ChatResponse |
| ChatService | process_message | return response | string |
| AgentExecutor | ainvoke | return result | dict |
| VehicleService | lookup_vehicle | return {...} | dict |
| QuoteService | get_quotes | return [...] | list[dict] |
| Redis Memory | get_memory | return memory | RedisChatMessageHistory |

---

## Debugging Entry Points

To add debug logging at key points:

```python
# 1. API Entry
backend/api/chat.py:26
logger.info(f"ðŸ”µ API: Received message from session {session_id}")

# 2. Service Entry
backend/services/chat_service.py:27
logger.info(f"ðŸŸ¢ Service: Processing message: {message[:50]}...")

# 3. Agent Entry
backend/services/chat_service.py:36
logger.info(f"ðŸŸ¡ Agent: Invoking with {len(chat_history)} history messages")

# 4. Tool Calls
backend/agent/tools.py:25
logger.info(f"ðŸ”§ Tool: Vehicle lookup called")

backend/agent/tools.py:45
logger.info(f"ðŸ”§ Tool: Get quote called")

# 5. Service Calls
backend/services/vehicle_service.py:31
logger.info(f"ðŸš— VehicleService: Lookup {make} {model} {year}")

backend/services/quote_service.py:31
logger.info(f"ðŸ’° QuoteService: Get quotes for {vehicle_make}")

# 6. Memory Operations
backend/services/chat_service.py:47
logger.info(f"ðŸ’¾ Memory: Saving user message")

backend/services/chat_service.py:48
logger.info(f"ðŸ’¾ Memory: Saving AI message")
```

This creates a trace like:
```
ðŸ”µ API: Received message from session 550e8400...
ðŸŸ¢ Service: Processing message: I need insurance for my 2020...
ðŸŸ¡ Agent: Invoking with 2 history messages
ðŸ”§ Tool: Vehicle lookup called
ðŸš— VehicleService: Lookup Toyota Camry 2020
ðŸ”§ Tool: Get quote called
ðŸ’° QuoteService: Get quotes for Toyota
ðŸ’¾ Memory: Saving user message
ðŸ’¾ Memory: Saving AI message
```
